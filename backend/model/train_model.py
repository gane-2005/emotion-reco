"""
Improved Model Training - Creates a better demo model with varied emotion predictions

This version creates a more realistic demo model that will show different emotions
for different audio files, even without the RAVDESS dataset.
"""

import os
import sys
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import joblib
import glob

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.feature_extraction import extract_features
import config


def load_ravdess_data(dataset_path):
    """Load RAVDESS dataset and extract features."""
    features = []
    labels = []
    
    emotion_map = {
        '01': 'Neutral',
        '02': 'Neutral',  # Calm -> Neutral
        '03': 'Happy',
        '04': 'Sad',
        '05': 'Angry',
        '06': 'Fear',
        '07': 'Disgust',
        '08': 'Surprise'
    }
    
    audio_files = glob.glob(os.path.join(dataset_path, '**', '*.wav'), recursive=True)
    
    if not audio_files:
        return None, None
    
    print(f"üìÇ Found {len(audio_files)} audio files")
    print("üîÑ Extracting features... This may take a few minutes.")
    
    for i, filepath in enumerate(audio_files):
        try:
            filename = os.path.basename(filepath)
            parts = filename.split('-')
            
            if len(parts) >= 3:
                emotion_code = parts[2]
                emotion = emotion_map.get(emotion_code, None)
                
                if emotion:
                    feature = extract_features(filepath)
                    
                    if feature is not None:
                        features.append(feature)
                        labels.append(emotion)
                        
                        if (i + 1) % 100 == 0:
                            print(f"  Processed {i + 1}/{len(audio_files)} files...")
        
        except Exception as e:
            continue
    
    print(f"‚úÖ Successfully extracted features from {len(features)} files")
    
    return np.array(features) if features else None, np.array(labels) if labels else None


def create_improved_demo_model():
    """
    Create an IMPROVED demo model with more realistic feature distributions.
    This will produce more varied predictions than the simple demo model.
    """
    print("\n‚ö†Ô∏è  Creating IMPROVED demo model with realistic feature distributions...")
    print("   NOTE: For best results, use the RAVDESS dataset.\n")
    
    np.random.seed(42)
    
    # Create more samples for better training
    n_samples_per_emotion = 150
    n_features = 40 + 12 + 128  # MFCC + Chroma + Mel
    
    X_list = []
    y_list = []
    
    # Create distinct feature patterns for each emotion
    emotion_patterns = {
        'Happy': {'mean': 2.0, 'std': 1.5},
        'Sad': {'mean': -1.5, 'std': 1.0},
        'Angry': {'mean': 3.0, 'std': 2.0},
        'Fear': {'mean': -0.5, 'std': 1.8},
        'Surprise': {'mean': 1.5, 'std': 2.5},
        'Disgust': {'mean': -2.0, 'std': 1.3},
        'Neutral': {'mean': 0.0, 'std': 1.0}
    }
    
    for emotion, pattern in emotion_patterns.items():
        # Generate features with emotion-specific distributions
        features = np.random.normal(
            loc=pattern['mean'],
            scale=pattern['std'],
            size=(n_samples_per_emotion, n_features)
        )
        
        # Add some feature-specific variations
        # MFCCs (first 40 features) have different characteristics
        features[:, :40] *= np.random.uniform(0.8, 1.2, 40)
        
        # Chroma features (next 12) have different range
        features[:, 40:52] *= np.random.uniform(0.5, 1.5, 12)
        
        # Mel features (last 128) have different scale
        features[:, 52:] *= np.random.uniform(0.9, 1.1, 128)
        
        X_list.append(features)
        y_list.extend([emotion] * n_samples_per_emotion)
    
    X = np.vstack(X_list)
    y = np.array(y_list)
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Train model with better parameters
    model = RandomForestClassifier(
        n_estimators=200,
        random_state=42,
        max_depth=30,
        min_samples_split=5,
        min_samples_leaf=2,
        class_weight='balanced',  # Better handling of class distribution
        n_jobs=-1
    )
    
    model.fit(X_train_scaled, y_train)
    
    # Evaluate
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"‚úÖ Improved Demo Model Accuracy: {accuracy:.2%}")
    print("\nüìä This model will show MORE VARIED predictions than the basic demo.")
    print("üí° For production quality (65-75% real accuracy), train on RAVDESS dataset.\n")
    
    return model, scaler


def train_model(dataset_path):
    """Train the emotion classification model."""
    
    print("\n" + "="*60)
    print("üéì Training Speech Emotion Analysis Model")
    print("="*60 + "\n")
    
    # Try to load real dataset first
    X, y = load_ravdess_data(dataset_path)
    
    # If no real data, create improved demo model
    if X is None or len(X) == 0:
        print(f"üìä No RAVDESS dataset found in {dataset_path}")
        model, scaler = create_improved_demo_model()
    else:
        # Train on real data
        print(f"\nüìä Dataset Statistics:")
        print(f"   Total samples: {len(X)}")
        print(f"   Feature dimensions: {X.shape[1]}")
        print(f"   Emotion distribution:")
        
        unique, counts = np.unique(y, return_counts=True)
        for emotion, count in zip(unique, counts):
            print(f"     - {emotion}: {count}")
        
        print("\nüîÄ Splitting data (80% train, 20% test)...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"   Training samples: {len(X_train)}")
        print(f"   Testing samples: {len(X_test)}")
        
        print("\nüìè Scaling features...")
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        print("\nüöÄ Training RandomForestClassifier...")
        model = RandomForestClassifier(
            n_estimators=200,
            random_state=42,
            max_depth=30,
            min_samples_split=5,
            min_samples_leaf=2,
            n_jobs=-1,
            verbose=1
        )
        
        model.fit(X_train_scaled, y_train)
        
        print("\nüìà Evaluating model...")
        y_pred = model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"\n‚úÖ Model Accuracy: {accuracy:.2%}")
        print("\nüìä Classification Report:")
        print(classification_report(y_test, y_pred))
    
    # Save model and scaler
    print("\nüíæ Saving model and scaler...")
    
    model_dir = os.path.dirname(config.MODEL_PATH)
    os.makedirs(model_dir, exist_ok=True)
    
    joblib.dump(model, config.MODEL_PATH)
    joblib.dump(scaler, config.SCALER_PATH)
    
    print(f"   Model saved to: {config.MODEL_PATH}")
    print(f"   Scaler saved to: {config.SCALER_PATH}")
    
    print("\n" + "="*60)
    print("‚úÖ Training Complete!")
    print("="*60)
    print("\nüéØ Next Steps:")
    print("   1. Restart the backend server")
    print("   2. Test with different audio files")
    print("   3. You should see MORE VARIED emotion predictions!")
    print("\nüí° To get 65-75% real accuracy:")
    print("   - Download RAVDESS: https://zenodo.org/record/1188976")
    print(f"   - Extract to: {dataset_path}")
    print("   - Run this script again\n")
    
    return model, scaler


if __name__ == '__main__':
    dataset_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        '..',
        'dataset',
        'RAVDESS'
    )
    
    print(f"Looking for dataset in: {dataset_path}")
    
    train_model(dataset_path)
